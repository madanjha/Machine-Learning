{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e768c0-80bf-4d0c-b8bc-f4e812903eb8",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Hands on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec3d5c-cdc8-497c-b317-a4a018eb64c4",
   "metadata": {},
   "source": [
    "### Step #1. `Sentence Segmentation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd60be19-5941-4ea5-bf5b-85acfaf0c9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\think\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\think\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"punkt_tab\")#for sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc730fc4-d73c-46c4-be25-56fb9f02aea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\think\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bac9e0b0-2fc6-4dad-bc01-879b5e4c5c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Dr. Smith went to Washington. He met Mr. Obama. It was productive.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c801918-1e84-4c1d-9565-4a7bb8912f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dr. Smith went to Washington. He met Mr. Obama. It was productive.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "425f425a-3851-45a6-a649-a12281f8f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d7a9007-4e88-4461-bf4a-5654bf462408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr. Smith went to Washington.', 'He met Mr. Obama.', 'It was productive.']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42c775c9-da6b-4ff1-821f-2f892c004173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b83431c-9abd-4d17-88e3-87fc4828220c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dr. Smith went to Washington.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8acf6abb-2105-4c0b-abfd-341b4a627a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "ww2_text = \"World War II, a global conflict that raged from 1939 to 1945, stands as the deadliest war in human history, claiming the lives of tens of millions. Fueled by aggressive expansionist policies of Axis powers like Nazi Germany, Imperial Japan, and Fascist Italy, the war engulfed nearly every part of the world. The conflict began with Germany's invasion of Poland, drawing in Allied powers such as Great Britain, France, the Soviet Union, and the United States. Characterized by unprecedented levels of violence, including the Holocaust, strategic bombing campaigns, and the use of atomic weapons, the war reshaped the political landscape, leading to the rise of new superpowers and the establishment of international organizations like the United Nations in an effort to prevent future global conflicts.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2118f2c2-4940-479e-988a-fceb746f4835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"World War II, a global conflict that raged from 1939 to 1945, stands as the deadliest war in human history, claiming the lives of tens of millions. Fueled by aggressive expansionist policies of Axis powers like Nazi Germany, Imperial Japan, and Fascist Italy, the war engulfed nearly every part of the world. The conflict began with Germany's invasion of Poland, drawing in Allied powers such as Great Britain, France, the Soviet Union, and the United States. Characterized by unprecedented levels of violence, including the Holocaust, strategic bombing campaigns, and the use of atomic weapons, the war reshaped the political landscape, leading to the rise of new superpowers and the establishment of international organizations like the United Nations in an effort to prevent future global conflicts.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww2_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "add5a462-4305-41e9-8f3f-f57e76eddd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ww2_text_sentences = sent_tokenize(ww2_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80701954-8285-4e41-b299-78ba43dd0f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['World War II, a global conflict that raged from 1939 to 1945, stands as the deadliest war in human history, claiming the lives of tens of millions.',\n",
       " 'Fueled by aggressive expansionist policies of Axis powers like Nazi Germany, Imperial Japan, and Fascist Italy, the war engulfed nearly every part of the world.',\n",
       " \"The conflict began with Germany's invasion of Poland, drawing in Allied powers such as Great Britain, France, the Soviet Union, and the United States.\",\n",
       " 'Characterized by unprecedented levels of violence, including the Holocaust, strategic bombing campaigns, and the use of atomic weapons, the war reshaped the political landscape, leading to the rise of new superpowers and the establishment of international organizations like the United Nations in an effort to prevent future global conflicts.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww2_text_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baf4108-eaa5-409c-a458-95665efac167",
   "metadata": {},
   "source": [
    "### Step #2. `Word Tokenization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90aa9743-36c7-460e-9e11-22504bbb3351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac7b6334-70a2-4d28-8db3-526d2dd7a2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dr. Smith went to Washington. He met Mr. Obama. It was productive.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1acacfd7-fef7-4426-9019-a36fa31ee4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenized_text = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff37d942-a35f-41a8-98ba-b319ef2cd2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.',\n",
       " 'Smith',\n",
       " 'went',\n",
       " 'to',\n",
       " 'Washington',\n",
       " '.',\n",
       " 'He',\n",
       " 'met',\n",
       " 'Mr.',\n",
       " 'Obama',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'productive',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ff2bd-d032-4d40-b8b5-2d5b35335fb3",
   "metadata": {},
   "source": [
    "### Task as the next step: `Character Tokenization`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dc67b0-47d4-4438-9b20-2c999622ba0a",
   "metadata": {},
   "source": [
    "### Step #3. `Stopword Removal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5cfbaf6-08b6-40ac-98f0-ad45ed59d46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\think\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44219146-5cd9-4ed9-8f59-492e4c1f0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "863236b4-6fa3-4946-9327-0868c6901fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8716c5-87f7-4d12-99e7-ef6c3ae0c64d",
   "metadata": {},
   "source": [
    "### `The Lord of the rings Movie Dialogue`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0dce20f-a681-43af-becd-d4dd4c0a5827",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"It's a dangerous business, Frodo, going out your door. You step onto the road, and if you don't keep your feet, there's no knowing where you might be swept off to.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77f4f39d-3142-443f-a780-013a19a403a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's a dangerous business, Frodo, going out your door. You step onto the road, and if you don't keep your feet, there's no knowing where you might be swept off to.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a781211e-ae8d-4b1f-82f2-6a93b9df4906",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd6846f3-e508-4408-844d-177e8bc3a7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'dangerous',\n",
       " 'business',\n",
       " ',',\n",
       " 'Frodo',\n",
       " ',',\n",
       " 'going',\n",
       " 'out',\n",
       " 'your',\n",
       " 'door',\n",
       " '.',\n",
       " 'You',\n",
       " 'step',\n",
       " 'onto',\n",
       " 'the',\n",
       " 'road',\n",
       " ',',\n",
       " 'and',\n",
       " 'if',\n",
       " 'you',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'keep',\n",
       " 'your',\n",
       " 'feet',\n",
       " ',',\n",
       " 'there',\n",
       " \"'s\",\n",
       " 'no',\n",
       " 'knowing',\n",
       " 'where',\n",
       " 'you',\n",
       " 'might',\n",
       " 'be',\n",
       " 'swept',\n",
       " 'off',\n",
       " 'to',\n",
       " '.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74951457-3cf6-4391-b9c5-14082a34d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = [ w for w in tokens if w.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e5bd2bb-b569-43ee-8125-5af0f41ddd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'s\",\n",
       " 'dangerous',\n",
       " 'business',\n",
       " ',',\n",
       " 'Frodo',\n",
       " ',',\n",
       " 'going',\n",
       " 'door',\n",
       " '.',\n",
       " 'step',\n",
       " 'onto',\n",
       " 'road',\n",
       " ',',\n",
       " \"n't\",\n",
       " 'keep',\n",
       " 'feet',\n",
       " ',',\n",
       " \"'s\",\n",
       " 'knowing',\n",
       " 'might',\n",
       " 'swept',\n",
       " '.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cac95cf8-7d9f-4eac-b2d8-2d14a71658f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 22)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens), len(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "877ed6c1-8ea9-47e8-90c7-46cac756f96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['It', \"'s\", 'a', 'dangerous', 'business', ',', 'Frodo', ',', 'going', 'out', 'your', 'door', '.', 'You', 'step', 'onto', 'the', 'road', ',', 'and', 'if', 'you', 'do', \"n't\", 'keep', 'your', 'feet', ',', 'there', \"'s\", 'no', 'knowing', 'where', 'you', 'might', 'be', 'swept', 'off', 'to', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "466404f6-5906-4fe2-be45-822fb462d5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stopword Removal: [\"'s\", 'dangerous', 'business', ',', 'Frodo', ',', 'going', 'door', '.', 'step', 'onto', 'road', ',', \"n't\", 'keep', 'feet', ',', \"'s\", 'knowing', 'might', 'swept', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"After Stopword Removal:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85202188-be49-429b-870b-6853c5080687",
   "metadata": {},
   "source": [
    "### Step # 4 & 5. `Stemming & Lemmatization`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c13e548-fe9f-4689-85a4-08ac3a8aefcf",
   "metadata": {},
   "source": [
    "### Stemming using `Porter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7b5bd6c7-a404-4864-b23a-114dacb880a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eb164318-69d8-4bf8-b73a-b7290e4db341",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"flies\", \"easily\", \"fairly\", \"happiness\", \"arguing\", \"automation\", \"studies\", \"organization\", \"studying\", \"everyone\", \"transforming\", \"stunning\", \"better\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "794bf92f-8f0a-4a83-b735-37f412481fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed = [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6d6ec354-a918-42af-9957-a08d6c3aad71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['running', 'flies', 'easily', 'fairly', 'happiness', 'arguing', 'automation', 'studies', 'organization', 'studying', 'everyone', 'transforming', 'stunning', 'better']\n",
      "After Stemming: ['run', 'fli', 'easili', 'fairli', 'happi', 'argu', 'autom', 'studi', 'organ', 'studi', 'everyon', 'transform', 'stun', 'better']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Words:\", words)\n",
    "print(\"After Stemming:\", stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a298ceef-2f9b-49c3-a487-7b6723f5c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "## Running stemming\n",
    "stemmed = [stemmer.stem(word) for word in words]\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time_ms = (end_time - start_time) * 1000  # convert to milliseconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "281d3ec0-2b31-4e13-aa4c-e9d8b9151c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['running', 'flies', 'easily', 'fairly', 'happiness', 'arguing', 'automation', 'studies', 'organization', 'studying', 'everyone', 'transforming', 'stunning', 'better']\n",
      "After Stemming: ['run', 'fli', 'easili', 'fairli', 'happi', 'argu', 'autom', 'studi', 'organ', 'studi', 'everyon', 'transform', 'stun', 'better']\n",
      "Time taken for stemming: 1.508 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Words:\", words)\n",
    "print(\"After Stemming:\", stemmed)\n",
    "print(f\"Time taken for stemming: {elapsed_time_ms:.3f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0108efd-fcc5-4559-9236-c6acca902f87",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7ed19844-cee9-4eb0-bb28-e9a0d523f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "924e5b8f-ede2-4dfa-bac8-40aee4da6b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\think\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\think\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Optional but helps with better lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7dae845-0b2f-4d23-b611-321d7af1d71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['running',\n",
       " 'flies',\n",
       " 'easily',\n",
       " 'fairly',\n",
       " 'happiness',\n",
       " 'arguing',\n",
       " 'automation',\n",
       " 'studies',\n",
       " 'organization',\n",
       " 'studying',\n",
       " 'everyone',\n",
       " 'transforming',\n",
       " 'stunning',\n",
       " 'better']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "338c75bb-813c-4284-9518-fdbaf1ac80a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "## Running lemmatization\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time_ms = (end_time - start_time) * 1000  # convert to milliseconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "83890bc6-d740-4223-bbbd-f38731665c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['running', 'flies', 'easily', 'fairly', 'happiness', 'arguing', 'automation', 'studies', 'organization', 'studying', 'everyone', 'transforming', 'stunning', 'better']\n",
      "After Lemmatization: ['running', 'fly', 'easily', 'fairly', 'happiness', 'arguing', 'automation', 'study', 'organization', 'studying', 'everyone', 'transforming', 'stunning', 'better']\n",
      "Time taken for lemmatization: 1.000 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Words:\", words)\n",
    "print(\"After Lemmatization:\", lemmatized)\n",
    "print(f\"Time taken for lemmatization: {elapsed_time_ms:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e887bcc0-c5cd-4a34-9f3e-0deefda12baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\", pos=wordnet.ADJ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd3052b-e43d-4230-8c4e-f29c2478c36e",
   "metadata": {},
   "source": [
    "- Conclusion: To avoid such kind of issues, **use `POS` tagging before lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b4de63-8ec2-40fe-a1b6-e763edf1e901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
